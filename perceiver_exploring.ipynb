{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "balanced-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from perceiver_pytorch import Perceiver\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "natural-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68710\n",
      "68710\n"
     ]
    }
   ],
   "source": [
    "model = Perceiver(\n",
    "    input_channels = 1,          # number of channels for each token of the input\n",
    "    input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    freq_base = 2,\n",
    "    depth = 3,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "                                 #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "    num_latents = 16,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    latent_dim = 32,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 2,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 4,         # number of dimensions per cross attention head\n",
    "    latent_dim_head = 4,        # number of dimensions per latent self attention head\n",
    "    num_classes = 10, # 91,          # output number of classes\n",
    "    attn_dropout = 0.,\n",
    "    ff_dropout = 0.,\n",
    "    weight_tie_layers = True,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "    fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "    self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
    ")\n",
    "\n",
    "# img = torch.randn(1, 28, 28, 1) # 1 imagenet image, pixelized\n",
    "\n",
    "# model(img).shape # (1, 1000)\n",
    "model_name = 'baseline_mnist'\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acting-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68710\n",
      "68710\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worth-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_data = torchvision.datasets.ImageNet('/home/gabriel/Documents/datasets/')\n",
    "# data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
    "#                                           batch_size=32,\n",
    "#                                           shuffle=True,\n",
    "#                                           num_workers=20)\n",
    "# train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "received-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('/home/gabriel/Documents/datasets/MNIST/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/home/gabriel/Documents/datasets/MNIST/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amino-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CocoMultilabelDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, coco_dataset):\n",
    "#         self.coco_dataset = coco_dataset\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         target = torch.zeros((91,))\n",
    "#         for det in self.coco_dataset[index][1]:\n",
    "#             if det['category_id'] > 90:\n",
    "#                 print(det)\n",
    "#             target[det['category_id']] = 1\n",
    "#         return (self.coco_dataset[index][0], target)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.coco_dataset)\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# batch_size = 16\n",
    "# coco_train = torchvision.datasets.CocoDetection(root = '/home/gabriel/Documents/datasets/COCO/train2017',\n",
    "#                                                 annFile = '/home/gabriel/Documents/datasets/COCO/annotations/instances_train2017.json',\n",
    "#                                                 transform=torchvision.transforms.Compose([\n",
    "#                                                     torchvision.transforms.Resize((112, 112)),\n",
    "#                                                     torchvision.transforms.ToTensor(), + 20\n",
    "#                                                     torchvision.transforms.Normalize(\n",
    "#                                                         (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                                 ]))\n",
    "# coco_train = CocoMultilabelDataset(coco_train)\n",
    "# split = int(0.9 * len(coco_train))\n",
    "# train_set, val_set = torch.utils.data.random_split(coco_train, [split, len(coco_train) - split])\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "# val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "# test_set = torchvision.datasets.CocoDetection(root = '/home/gabriel/Documents/datasets/COCO/val2017',\n",
    "#                                               annFile = '/home/gabriel/Documents/datasets/COCO/annotations/instances_val2017.json',\n",
    "#                                               transform=torchvision.transforms.Compose([\n",
    "#                                                   torchvision.transforms.Resize((112, 112)),\n",
    "#                                                   torchvision.transforms.ToTensor(),\n",
    "#                                                   torchvision.transforms.Normalize(\n",
    "#                                                       (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                               ]))\n",
    "# test_set = CocoMultilabelDataset(test_set)\n",
    "# test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "# print(len(coco_train))\n",
    "# im = torch.transpose(torch.transpose(coco_train[1][0], 0, 1), 1, 2)\n",
    "# plt.imshow(im.cpu().numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "steady-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# batch_size = 16\n",
    "\n",
    "# train_set = torchvision.datasets.CelebA(root = '/home/gabriel/Documents/datasets/CelebA/Img/img_celeba.7z', split='train',\n",
    "#                                         transform=torchvision.transforms.Compose([\n",
    "#                                            torchvision.transforms.Resize((112, 112)),\n",
    "#                                            torchvision.transforms.ToTensor(),\n",
    "#                                            torchvision.transforms.Normalize(\n",
    "#                                                (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                        ]))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "# val_set = torchvision.datasets.CelebA(root = '/home/gabriel/Documents/datasets/CelebA', split='valid',\n",
    "#                                       transform=torchvision.transforms.Compose([\n",
    "#                                           torchvision.transforms.Resize((112, 112)),\n",
    "#                                           torchvision.transforms.ToTensor(),\n",
    "#                                           torchvision.transforms.Normalize(\n",
    "#                                             (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                       ]))\n",
    "# val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "# test_set = torchvision.datasets.CelebA(root = '/home/gabriel/Documents/datasets/CelebA', split='test',\n",
    "#                                        transform=torchvision.transforms.Compose([\n",
    "#                                            torchvision.transforms.Resize((112, 112)),\n",
    "#                                            torchvision.transforms.ToTensor(),\n",
    "#                                            torchvision.transforms.Normalize(\n",
    "#                                                (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                        ]))\n",
    "# test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e4063d16164aaab6d06eb5e7f4489c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.526316 \tValidation Loss: 0.904048\n",
      "Validation loss decreased (100000.000000 --> 0.904048).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d3360180e94c4ba13aec5e5c7e0b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.764194 \tValidation Loss: 0.623925\n",
      "Validation loss decreased (0.904048 --> 0.623925).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7b82853c5e44c0aec1a03cfa34fd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.572412 \tValidation Loss: 0.513249\n",
      "Validation loss decreased (0.623925 --> 0.513249).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4945f9214b26431889b3aedafe799427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.452697 \tValidation Loss: 0.443577\n",
      "Validation loss decreased (0.513249 --> 0.443577).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad4a61ab18044e49a9d6f3d9e35d2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.371965 \tValidation Loss: 0.350043\n",
      "Validation loss decreased (0.443577 --> 0.350043).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9223b242337c4ba9be06b7e4c2c21ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 0.315183 \tValidation Loss: 0.303256\n",
      "Validation loss decreased (0.350043 --> 0.303256).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92f2f57399545138d755e21eba96090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 0.274187 \tValidation Loss: 0.257770\n",
      "Validation loss decreased (0.303256 --> 0.257770).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581005ff6a8546419bab015812aa14cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 0.243919 \tValidation Loss: 0.228733\n",
      "Validation loss decreased (0.257770 --> 0.228733).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230dceb7c7664be095c12667fc8aa7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 0.220620 \tValidation Loss: 0.223711\n",
      "Validation loss decreased (0.228733 --> 0.223711).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f289fc0e58d84e8280b5031669b006c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 0.204336 \tValidation Loss: 0.211395\n",
      "Validation loss decreased (0.223711 --> 0.211395).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c3b9692b264dd1a8e5cf6a8e99b8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.191705 \tValidation Loss: 0.201283\n",
      "Validation loss decreased (0.211395 --> 0.201283).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb498792a8414fbca53de41eac69e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 0.178805 \tValidation Loss: 0.190615\n",
      "Validation loss decreased (0.201283 --> 0.190615).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c589cecfd64d448f597384f5fb7a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 0.169314 \tValidation Loss: 0.182885\n",
      "Validation loss decreased (0.190615 --> 0.182885).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386a1450f3634111b2480ffe6c5696e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "# checkpoint = torch.load(f'checkpoints/{model_name}/best_checkpoint.pt')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "val_loss_min = 100000.0\n",
    "\n",
    "writer = SummaryWriter()\n",
    "if not os.path.isdir(f'checkpoints/{model_name}'):\n",
    "    os.mkdir(f'checkpoints/{model_name}')\n",
    "\n",
    "now = datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "    \n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    loss_count = 0\n",
    "    \n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{max_epochs}\")):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        X_batch = torch.transpose(torch.transpose(X_batch, 1, 2), 2, 3)\n",
    "#         from matplotlib import pyplot as plt\n",
    "#         plt.imshow(X_batch[0].cpu().numpy())\n",
    "#         plt.show()\n",
    "#         print(X_batch.shape, y_batch.shape)\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data\n",
    "        writer.add_scalars(\"losses_step\", {\"train_loss\": loss.data}, epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(val_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            X_batch = torch.transpose(torch.transpose(X_batch, 1, 2), 2, 3)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.data\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    writer.add_scalars(\"losses_step\", {\"val_loss\": val_loss}, (epoch + 1) * len(train_loader) - 1)\n",
    "    \n",
    "    writer.add_scalars(\"losses_epoch\", {\"train_loss\": train_loss}, epoch)\n",
    "    writer.add_scalars(\"losses_epoch\", {\"val_loss\": val_loss}, epoch)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        val_loss\n",
    "    ))\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'valid_loss_min': val_loss,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, f'checkpoints/{model_name}/checkpoint_{now}.pt')\n",
    "    if val_loss <= val_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
    "        torch.save(checkpoint, f'checkpoints/{model_name}/best_checkpoint_{now}.pt')\n",
    "        val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_acc(y_pred, y_true):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(len(y_true)), desc=f\"Computing accuracy\"):\n",
    "        pred = y_pred[i]\n",
    "        true = y_true[i]\n",
    "        for j in range(len(true)):\n",
    "            total += 1\n",
    "            if true[j] == round(float(pred[j])):\n",
    "                correct += 1\n",
    "                \n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# def multilabel_prec(y_pred, y_true):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i in tqdm(range(len(y_true)), desc=f\"Computing precision\"):\n",
    "#         pred = y_pred[i]\n",
    "#         true = y_true[i]\n",
    "#         for j in range(len(true)):\n",
    "#             total += 1\n",
    "#             if true[j] == 1:\n",
    "#                 if round(float(pred[j])) == 1:\n",
    "#                     correct += 1\n",
    "#             elif round(float(pred[j])) == 0:\n",
    "#                 correct += 1\n",
    "                \n",
    "#     return correct / total\n",
    "\n",
    "\n",
    "# def multilabel_recall(y_pred, y_true):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i in tqdm(range(len(y_true)), desc=f\"Computing recall\"):\n",
    "#         pred = y_pred[i]\n",
    "#         true = y_true[i]\n",
    "#         for j in range(len(true)):\n",
    "#             total += 1\n",
    "#             if true[j] == 1:\n",
    "#                 if round(float(pred[j])) == 1:\n",
    "#                     correct += 1\n",
    "#             elif round(float(pred[j])) == 0:\n",
    "#                 correct += 1\n",
    "                \n",
    "#     return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-scott",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model.to(device)\n",
    "# checkpoint = torch.load(f'checkpoints/{model_name}/best_checkpoint.pt')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "m = nn.Softmax(dim=1)\n",
    "# m = nn.Sigmoid()\n",
    "y_pred_extended = []\n",
    "y_true_extended = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(tqdm(test_loader, desc=f\"Inference\")):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        X_batch = torch.transpose(torch.transpose(X_batch, 1, 2), 2, 3)\n",
    "        y_pred = model(X_batch).cpu()\n",
    "        y_pred = m(y_pred)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_pred_extended.extend(y_pred)\n",
    "        y_true_extended.extend(y_batch.cpu())\n",
    "        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(25,25))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# single label\n",
    "accuracy = accuracy_score(y_pred_extended, y_true_extended)\n",
    "cr = classification_report(y_true_extended, y_pred_extended)\n",
    "print(accuracy)\n",
    "print(cr)\n",
    "cnf_matrix = confusion_matrix(y_true_extended, y_pred_extended)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[str(i) for i in list(range(10))], title = ('Confusion Matrix'))\n",
    "plt.show()\n",
    "\n",
    "# # multilabel\n",
    "# accuracy = multilabel_acc(y_pred_extended, y_true_extended)\n",
    "# # precision = multilabel_prec(y_pred_extended, y_true_extended)\n",
    "# # recall = multilabel_recall(y_pred_extended, y_true_extended)\n",
    "# print(accuracy)  # , precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Perceiver(\n",
    "#     input_channels = 3,          # number of channels for each token of the input\n",
    "#     input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "#     num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
    "#     max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "#     freq_base = 2,\n",
    "#     depth = 2,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "#                                  #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "#     num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "#     latent_dim = 256,            # latent dimension\n",
    "#     cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "#     latent_heads = 2,            # number of heads for latent self attention, 8\n",
    "#     cross_dim_head = 32,         # number of dimensions per cross attention head\n",
    "#     latent_dim_head = 32,        # number of dimensions per latent self attention head\n",
    "#     num_classes = 1000,          # output number of classes\n",
    "#     attn_dropout = 0.,\n",
    "#     ff_dropout = 0.,\n",
    "#     weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "#     fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "#     self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
    "# )\n",
    "\n",
    "# print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers.append(model.layers[1])\n",
    "# print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-powder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
