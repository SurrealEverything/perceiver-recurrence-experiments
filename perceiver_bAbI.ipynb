{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brave-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torchvision\n",
    "import torchtext\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from perceiver_pytorch import Perceiver\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from babi.babi_loader import BabiDataset, pad_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retired-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from perceiver_pytorch import PerceiverIO\n",
    "\n",
    "model = PerceiverIO(\n",
    "    dim = 13,                    # dimension of sequence to be encoded\n",
    "    queries_dim = 32,            # dimension of decoder queries\n",
    "    logits_dim = 50,            # dimension of final logits\n",
    "    depth = 6,                   # depth of net\n",
    "    num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    latent_dim = 512,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
    "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
    "    weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "    self_per_cross_attn = 2,     # number of self attention blocks per cross attention\n",
    ")\n",
    "\n",
    "# seq = torch.randn(1, 512, 32)\n",
    "# queries = torch.randn(1, 128, 32)\n",
    "\n",
    "# logits = model(seq, queries = queries) # (1, 128, 100) - (batch, decoder seq, logits dim)\n",
    "\n",
    "# add bAbI query as latent query\n",
    "# only one query/question and answer per forward pass\n",
    "\n",
    "seq = torch.randn(1, 70, 13) # (batch, input_sequence_length, input_sequence_dim/dim)\n",
    "queries = torch.randn(1, 1, 32) # (batch, output_sequence_length, queries_dim) - probably should be learned\n",
    "\n",
    "logits = model(seq, queries = queries) # (batch, output_sequence_length, output_sequence_dim/logits dim)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 13 13\n",
      "{2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49}\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "max_epochs = 2\n",
    "\n",
    "max_con_num_sentences, max_con_num_words, max_qus_num_words = 0, 0, 0\n",
    "ans_lis = []\n",
    "# c=0\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# print(device)\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "\n",
    "for task_id in range(1, 21):\n",
    "    dset = BabiDataset(task_id, \n",
    "                       ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "                       vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "    vocab_size = len(dset.QA.VOCAB)\n",
    "    dset.set_mode('train')\n",
    "    train_loader = DataLoader(\n",
    "        dset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate\n",
    "    )\n",
    "    \n",
    "#     c += len(train_loader.dataset)\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "        contexts, questions, answers = data\n",
    "        contexts = Variable(contexts.long().cuda())\n",
    "        questions = Variable(questions.long().cuda())\n",
    "        answers = Variable(answers.cuda())\n",
    "#         print(contexts.shape, questions.shape, answers.shape)\n",
    "        _, con_num_sentences, con_num_words = contexts.shape\n",
    "        _, qus_num_words = questions.shape\n",
    "        if con_num_sentences > max_con_num_sentences:\n",
    "            max_con_num_sentences = con_num_sentences\n",
    "        if con_num_words > max_con_num_words:\n",
    "            max_con_num_words = con_num_words\n",
    "        if qus_num_words > max_qus_num_words:\n",
    "            max_qus_num_words = qus_num_words\n",
    "            \n",
    "        for ans in answers:\n",
    "            ans_lis.append(int(ans))\n",
    "        \n",
    "#     dset.set_mode('valid')\n",
    "#     valid_loader = DataLoader(\n",
    "#         dset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate\n",
    "#     )\n",
    "     \n",
    "    dset.set_mode('test')\n",
    "    train_loader = DataLoader(\n",
    "        dset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate\n",
    "    )\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        contexts, questions, answers = data\n",
    "        contexts = Variable(contexts.long().cuda())\n",
    "        questions = Variable(questions.long().cuda())\n",
    "        answers = Variable(answers.cuda())\n",
    "        for ans in answers:\n",
    "            ans_lis.append(int(ans))\n",
    "            \n",
    "    dset.set_mode('valid')\n",
    "    train_loader = DataLoader(\n",
    "        dset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate\n",
    "    )\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        contexts, questions, answers = data\n",
    "        contexts = Variable(contexts.long().cuda())\n",
    "        questions = Variable(questions.long().cuda())\n",
    "        answers = Variable(answers.cuda())\n",
    "        for ans in answers:\n",
    "            ans_lis.append(int(ans))\n",
    "        \n",
    "ans_lis = set(ans_lis)\n",
    "print(max_con_num_sentences, max_con_num_words, max_qus_num_words)\n",
    "print(ans_lis)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adequate-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # checkpoint = torch.load(f'checkpoints/{model_name}/best_checkpoint.pt')\n",
    "# # model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# # criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# val_loss_min = 100000.0\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "# if not os.path.isdir(f'checkpoints/{model_name}'):\n",
    "#     os.mkdir(f'checkpoints/{model_name}')\n",
    "\n",
    "# now = datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "    \n",
    "# for epoch in range(max_epochs):\n",
    "#     train_loss = 0.0\n",
    "#     val_loss = 0.0\n",
    "    \n",
    "#     model.train()\n",
    "#     loss_sum = 0\n",
    "#     loss_count = 0\n",
    "    \n",
    "#     for batch_idx, (X_batch, y_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{max_epochs}\")):\n",
    "#         optimizer.zero_grad()\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         X_batch = torch.transpose(torch.transpose(X_batch, 1, 2), 2, 3)\n",
    "# #         from matplotlib import pyplot as plt\n",
    "# #         plt.imshow(X_batch[0].cpu().numpy())\n",
    "# #         plt.show()\n",
    "# #         print(X_batch.shape, y_batch.shape)\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.data\n",
    "#         writer.add_scalars(\"losses_step\", {\"train_loss\": loss.data}, epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (X_batch, y_batch) in enumerate(val_loader):\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             X_batch = torch.transpose(torch.transpose(X_batch, 1, 2), 2, 3)\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             val_loss += loss.data\n",
    "\n",
    "#     train_loss = train_loss / len(train_loader)\n",
    "#     val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "#     writer.add_scalars(\"losses_step\", {\"val_loss\": val_loss}, (epoch + 1) * len(train_loader) - 1)\n",
    "    \n",
    "#     writer.add_scalars(\"losses_epoch\", {\"train_loss\": train_loss}, epoch)\n",
    "#     writer.add_scalars(\"losses_epoch\", {\"val_loss\": val_loss}, epoch)\n",
    "    \n",
    "    \n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "#         epoch+1, \n",
    "#         train_loss,\n",
    "#         val_loss\n",
    "#     ))\n",
    "    \n",
    "#     checkpoint = {\n",
    "#         'epoch': epoch + 1,\n",
    "#         'valid_loss_min': val_loss,\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'optimizer': optimizer.state_dict(),\n",
    "#     }\n",
    "    \n",
    "#     torch.save(checkpoint, f'checkpoints/{model_name}/checkpoint_{now}.pt')\n",
    "#     if val_loss <= val_loss_min:\n",
    "#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
    "#         torch.save(checkpoint, f'checkpoints/{model_name}/best_checkpoint_{now}.pt')\n",
    "#         val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corrected-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")aset\n",
    "# model.to(device)\n",
    "# # checkpoint = torch.load(f'checkpoints/{model_name}/best_checkpoint.pt')\n",
    "# # model.load_state_dict(checkpoint['state_dict'])\n",
    "# model.eval()\n",
    "# m = nn.Softmax(dim=1)\n",
    "\n",
    "# dset.set_mode('test')\n",
    "# test_loader = DataLoader(\n",
    "#     dset, batch_size=100, shuffle=False, collate_fn=pad_collate\n",
    "# )\n",
    "\n",
    "# # m = nn.Sigmoid()\n",
    "# y_pred_extended = []\n",
    "# y_true_extended = []\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (X_batch, y_batch) in enumerate(tqdm(test_loader, desc=f\"Inference\")):\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         X_batch = torch.transpose(torch.transpose(X_batch, 1, 2), 2, 3)\n",
    "#         y_pred = model(X_batch).cpu()\n",
    "#         y_pred = m(y_pred)\n",
    "#         y_pred = np.argmax(y_pred, axis=1)\n",
    "#         y_pred_extended.extend(y_pred)\n",
    "#         y_true_extended.extend(y_batch.cpu())\n",
    "        \n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         #print(\"Normalized confusion matrix\")\n",
    "#     #else:\n",
    "#         #print('Confusion matrix, without normalization')\n",
    "\n",
    "#     #print(cm)\n",
    "    \n",
    "#     plt.figure(figsize=(25,25))\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# # single label\n",
    "# accuracy = accuracy_score(y_pred_extended, y_true_extended)\n",
    "# cr = classification_report(y_true_extended, y_pred_extended)\n",
    "# print(accuracy)\n",
    "# print(cr)\n",
    "# cnf_matrix = confusion_matrix(y_true_extended, y_pred_extended)\n",
    "# # Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[str(i) for i in list(range(10))], title = ('Confusion Matrix'))\n",
    "# plt.show()\n",
    "\n",
    "# # # multilabel\n",
    "# # accuracy = multilabel_acc(y_pred_extended, y_true_extended)\n",
    "# # # precision = multilabel_prec(y_pred_extended, y_true_extended)\n",
    "# # # recall = multilabel_recall(y_pred_extended, y_true_extended)\n",
    "# # print(accuracy)  # , precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-imagination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
