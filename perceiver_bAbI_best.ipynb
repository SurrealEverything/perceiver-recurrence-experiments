{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brave-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from perceiver_pytorch import Perceiver\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from babi_joint import BabiDataset, pad_collate_question\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from glob import glob\n",
    "from perceiver_pytorch.perceiver_io_best import PerceiverIObAbI, PonderLoss\n",
    "import time\n",
    "# from ranger21 import Ranger21\n",
    "# from ranger import Ranger\n",
    "# from warmup_scheduler import GradualWarmupScheduler\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104314\n"
     ]
    }
   ],
   "source": [
    "# UT pondernet 1470003\n",
    "\n",
    "model = PerceiverIObAbI(\n",
    "    num_tokens = 179,\n",
    "    context_max_seq_len = 783,\n",
    "    question_max_seq_len = 13,\n",
    "    depth = 10,\n",
    "    dim = 128,\n",
    "    input_queries_dim = 128,\n",
    "    output_queries_dim = 128,\n",
    "    latent_queries_dim = 128,\n",
    "    num_output_queries = 2,\n",
    "    num_latents = 64,\n",
    "    logits_dim = 60,\n",
    "    input_cross_heads = 4,\n",
    "    cross_heads = 4,\n",
    "    latent_heads = 8,\n",
    "    input_cross_dim_head = 64,\n",
    "    cross_dim_head = 64,\n",
    "    latent_queries_dim_head = 64\n",
    ")\n",
    "\n",
    "model_name = 'perceiverIO_bAbi_best_fixed'\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(babi_dataset) train 180000\n",
      "vocab_size 178\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a70d704165415f8c9dd53f9d3ed0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.319617 \tValidation Loss: 1.656367\n",
      "Validation loss decreased (inf --> 1.656367).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098c0ebf53e84aa7a1eb75f2e2972366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 1.571801 \tValidation Loss: 1.491169\n",
      "Validation loss decreased (1.656367 --> 1.491169).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f59ad03cee49cebed4e445bb0ab5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 1.470107 \tValidation Loss: 1.429234\n",
      "Validation loss decreased (1.491169 --> 1.429234).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9756814777472e86511ccc2a83d2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 1.409352 \tValidation Loss: 1.375389\n",
      "Validation loss decreased (1.429234 --> 1.375389).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2a7ac6b3544ca0899d523be3c73c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 1.367066 \tValidation Loss: 1.343726\n",
      "Validation loss decreased (1.375389 --> 1.343726).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8915d8e079c9422ab5a782df46fb5221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 1.333918 \tValidation Loss: 1.340359\n",
      "Validation loss decreased (1.343726 --> 1.340359).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccc386215af43efba6071888af39817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 1.311187 \tValidation Loss: 1.300738\n",
      "Validation loss decreased (1.340359 --> 1.300738).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dad5115581a4cc08155b452a757bef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 1.292607 \tValidation Loss: 1.294313\n",
      "Validation loss decreased (1.300738 --> 1.294313).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d66b4de668457093f327e17f129802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 1.278429 \tValidation Loss: 1.272702\n",
      "Validation loss decreased (1.294313 --> 1.272702).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5ead1f7e744288bd09b74fe130683c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/200:   0%|          | 0/1407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 200\n",
    "\n",
    "babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "                           vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "vocab_size = len(babi_dataset.QA.VOCAB)\n",
    "\n",
    "print('len(babi_dataset) train', len(babi_dataset))\n",
    "print('vocab_size', vocab_size)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model.to(device)\n",
    "# model.half()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "depth = 20\n",
    "criterion = PonderLoss(nn.CrossEntropyLoss(reduction='none'), 1/5, depth, 0.01).to(device) # .half()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "# optimizer = Ranger21(model.parameters(), lr=3e-5, num_epochs=max_epochs, num_batches_per_epoch=len(babi_dataset)//batch_size+1, use_madgrad=True)\n",
    "# optimizer = Ranger(model.parameters(), lr=3e-5)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "# scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=5, after_scheduler=scheduler)\n",
    "\n",
    "val_loss_min = float('inf')\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "if not os.path.isdir(f'checkpoints/{model_name}'):\n",
    "    os.mkdir(f'checkpoints/{model_name}')\n",
    "now = datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    loss_sum = 0\n",
    "    loss_count = 0\n",
    "    model.train()\n",
    "    babi_dataset.set_mode('train')\n",
    "    train_loader = DataLoader(\n",
    "        babi_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_question\n",
    "    )\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{max_epochs}\")):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        contexts, questions, answers, tasks = data\n",
    "        contexts = contexts.long().to(device)#.half()\n",
    "        questions = questions.long().to(device)#.half()\n",
    "        answers = answers.to(device)#.half()\n",
    "        \n",
    "#         with torch.cuda.amp.autocast():\n",
    "        p, y_hat, p_sampled, y_hat_sampled = model(contexts, questions)\n",
    "        loss = criterion(p, y_hat, answers)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        writer.add_scalars(\"losses_step\", {\"train_loss\": loss.item() / contexts.size(0)}, epoch * len(train_loader) + batch_idx)\n",
    "                \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    writer.add_scalars(\"losses_epoch\", {\"train_loss\": train_loss}, epoch)\n",
    "    train_loader_len = len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    babi_dataset.set_mode('valid')\n",
    "    val_loader = DataLoader(\n",
    "        babi_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_question\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "        \n",
    "            contexts, questions, answers, tasks = data\n",
    "            contexts = contexts.long().to(device)\n",
    "            questions = questions.long().to(device)\n",
    "            answers = answers.to(device)\n",
    "        \n",
    "            p, y_hat, p_sampled, y_hat_sampled = model(contexts, questions)\n",
    "            loss = criterion(p, y_hat, answers)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "#     scheduler.step(epoch, val_loss)\n",
    "    writer.add_scalars(\"losses_step\", {\"val_loss\": val_loss}, (epoch + 1) * train_loader_len - 1)    \n",
    "    writer.add_scalars(\"losses_epoch\", {\"val_loss\": val_loss}, epoch)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        val_loss\n",
    "    ))\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'valid_loss_min': val_loss,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, f'checkpoints/{model_name}/checkpoint_{now}.pt')\n",
    "    if val_loss <= val_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
    "        torch.save(checkpoint, f'checkpoints/{model_name}/best_checkpoint_{now}.pt')\n",
    "        val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-hypothesis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "# now='29_08_2021__23_49_17'\n",
    "checkpoint_name = f'checkpoints/{model_name}/best_checkpoint_{now}.pt'\n",
    "checkpoint = torch.load(checkpoint_name)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "                           vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "babi_dataset.set_mode('test')\n",
    "test_loader = DataLoader(\n",
    "    babi_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_question\n",
    ")\n",
    "output_vocab = eval('{0: 2, 1: 6, 2: 9, 3: 15, 4: 16, 5: 19, 6: 20, 7: 22, 8: 25, 9: 154, 10: 155, 11: 156, 12: 29, 13: 157, 14: 158, 15: 160, 16: 161, 17: 162, 18: 159, 19: 164, 20: 163, 21: 165, 22: 167, 23: 43, 24: 45, 25: 173, 26: 175, 27: 177, 28: 49, 29: 54, 30: 55, 31: 60, 32: 61, 33: 62, 34: 63, 35: 64, 36: 65, 37: 66, 38: 67, 39: 68, 40: 69, 41: 70, 42: 71, 43: 72, 44: 73, 45: 74, 46: 75, 47: 76, 48: 80, 49: 82, 50: 83, 51: 84, 52: 106, 53: 108, 54: 112, 55: 113, 56: 117, 57: 120, 58: 126, 59: 127}')\n",
    "y_pred_extended = []\n",
    "y_true_extended = []\n",
    "y_pred_task = [[] for _ in range(20)]\n",
    "y_true_task = [[] for _ in range(20)]\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(tqdm(test_loader, desc=f\"Inference\")):\n",
    "        contexts, questions, answers, tasks = data\n",
    "        contexts = contexts.long().to(device)\n",
    "        questions = questions.long().to(device)\n",
    "        answers = answers.to(device)\n",
    "\n",
    "        p, y_hat, p_sampled, y_hat_sampled = model(contexts, questions)\n",
    "        # We consider a task successfully passed if ≥ 95% accuracy is obtained.\n",
    "        y_pred = m(y_hat_sampled.cpu())\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_pred = [babi_dataset.QA.IVOCAB[output_vocab[int(i)]] for i in y_pred]\n",
    "        y_true = [babi_dataset.QA.IVOCAB[output_vocab[int(i)]] for i in answers.cpu()]\n",
    "        y_pred_extended.extend(y_pred)\n",
    "        y_true_extended.extend(y_true)\n",
    "        for i in range(len(y_pred)):\n",
    "            y_pred_task[int(tasks[i]) - 1].append(y_pred[i])\n",
    "            y_true_task[int(tasks[i]) - 1].append(y_true[i])\n",
    "end = time.time()\n",
    "\n",
    "        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')p.shape\n",
    "\n",
    "    #print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(25,25))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "accuracy = accuracy_score(y_pred_extended, y_true_extended)\n",
    "cr = classification_report(y_true_extended, y_pred_extended)\n",
    "print(accuracy)\n",
    "print(cr)\n",
    "task_acc = []\n",
    "passed_tasks = []\n",
    "for i in range(20):\n",
    "    task_acc.append(accuracy_score(y_pred_task[i], y_true_task[i]))\n",
    "    passed_tasks.append(1 if task_acc[i] >= 0.95 else 0)\n",
    "print(task_acc)\n",
    "print('passed_tasks:', passed_tasks)\n",
    "print('no. passed_tasks:', sum(passed_tasks))\n",
    "labels = list(set(y_true_extended).intersection(set(y_true_extended)))\n",
    "cnf_matrix = confusion_matrix(y_true_extended, y_pred_extended, labels)\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=labels, title = ('Confusion Matrix'))\n",
    "plt.show()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thop import profile\n",
    "# from thop import clever_format\n",
    "\n",
    "# macs, params = profile(model, inputs=(\n",
    "#                         torch.randint(1, size=(1, 910)).type(torch.LongTensor).to(device), \n",
    "#                         torch.randint(1, size=(1, 13)).type(torch.LongTensor).to(device)))  # , \n",
    "# #                         custom_ops={YourModule: count_your_model})\n",
    "\n",
    "# macs, params = clever_format([macs, params], \"%.3f\")\n",
    "# print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers.append(model.layers[1])\n",
    "# print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-reservoir",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=128\n",
    "# babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "#                            vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "# vocab_size = len(babi_dataset.QA.VOCAB)\n",
    "# babi_dataset.set_mode('valid')\n",
    "# train_loader = DataLoader(\n",
    "#     babi_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate\n",
    "# )\n",
    "# max_context = 0\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, answers, tasks = data\n",
    "# #     print(contexts.shape, answers.shape)\n",
    "#     for b in range(contexts.shape[0]):\n",
    "# #         print(f'Task {int(tasks[b])}:')\n",
    "#         for i, w in enumerate(contexts[b]):\n",
    "#             if w != 0:\n",
    "#                 if i + 1 == contexts.shape[1]:\n",
    "#                     max_context = i\n",
    "#                     if i > 600:\n",
    "#                         print(f'Task {int(tasks[b])}:')\n",
    "#                         for i, w in enumerate(contexts[b]):\n",
    "#                             if w != 0:\n",
    "#                                 print(babi_dataset.QA.IVOCAB[int(w)], end=\" \")\n",
    "#                             else:\n",
    "#                                 print(f'({i} words)')\n",
    "#                                 break\n",
    "# #                 print(babi_dataset.QA.IVOCAB[int(w)], end=\" \")\n",
    "                \n",
    "#             else:\n",
    "# #                 print(f'({i} words)')\n",
    "#                 if i > max_context:\n",
    "#                     max_context = i\n",
    "#                     if i > 600:\n",
    "#                         print(f'Task {int(tasks[b])}:')\n",
    "#                         for i, w in enumerate(contexts[b]):\n",
    "#                             if w != 0:\n",
    "#                                 print(babi_dataset.QA.IVOCAB[int(w)], end=\" \")\n",
    "#                             else:\n",
    "#                                 print(f'({i} words)')\n",
    "#                                 break\n",
    "#                 break\n",
    "# #         print('\\n')\n",
    "# #         print(babi_dataset.QA.IVOCAB[int(answers[b])])\n",
    "# #         print('\\n')\n",
    "# #     break\n",
    "# print(max_context)\n",
    "\n",
    "# batch_size=128\n",
    "# babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "#                            vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "# vocab_size = len(babi_dataset.QA.VOCAB)\n",
    "# babi_dataset.set_mode('train')\n",
    "# train_loader = DataLoader(\n",
    "#     babi_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_question\n",
    "# )\n",
    "# answer_list = []\n",
    "# max_question = 0\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, questions, answers, tasks = data\n",
    "#     answer_list += answers.tolist()\n",
    "#     for b in range(questions.shape[0]):\n",
    "#         for i, w in enumerate(questions[b]):\n",
    "#             if w == 0:\n",
    "#                 if i > max_question or i + 1 == questions.shape[1]:\n",
    "#                     max_question = i\n",
    "\n",
    "# babi_dataset.set_mode('test')\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, questions, answers, tasks = data\n",
    "#     answer_list += answers.tolist()\n",
    "#     for b in range(questions.shape[0]):\n",
    "#         for i, w in enumerate(questions[b]):\n",
    "#             if w == 0:\n",
    "#                 if i > max_question or i + 1 == questions.shape[1]:\n",
    "#                     max_question = i\n",
    "                    \n",
    "# babi_dataset.set_mode('valid')\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, questions, answers, tasks = data\n",
    "#     answer_list += answers.tolist()\n",
    "#     for b in range(questions.shape[0]):\n",
    "#         for i, w in enumerate(questions[b]):\n",
    "#             if w == 0:\n",
    "#                 if i > max_question or i + 1 == questions.shape[1]:\n",
    "#                     max_question = i\n",
    "    \n",
    "# print(len(set(answer_list)), set(answer_list))\n",
    "# print(max_question)\n",
    "# output_vocab = dict()\n",
    "# for i, w in enumerate(set(answer_list)):\n",
    "#     output_vocab[i] = w\n",
    "# print(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-jefferson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
