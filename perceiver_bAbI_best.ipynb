{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brave-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from perceiver_pytorch import Perceiver\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from babi_joint import BabiDataset, pad_collate_question\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from glob import glob\n",
    "from perceiver_pytorch.perceiver_io_best import PerceiverIObAbI, PonderLoss\n",
    "import time\n",
    "# from ranger21 import Ranger21\n",
    "# from ranger import Ranger\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018301\n"
     ]
    }
   ],
   "source": [
    "# UT pondernet 1470003\n",
    "\n",
    "model = PerceiverIObAbI(\n",
    "    num_tokens = 179,\n",
    "    context_max_seq_len = 783,\n",
    "    question_max_seq_len = 13,\n",
    "    depth = 20,\n",
    "    dim = 128,\n",
    "    input_queries_dim = 128,\n",
    "    output_queries_dim = 128,\n",
    "    latent_queries_dim = 128,\n",
    "    num_output_queries = 4,\n",
    "    num_latents = 64,\n",
    "    logits_dim = 60,\n",
    "    input_cross_heads = 2,\n",
    "    cross_heads = 2,\n",
    "    latent_heads = 8,\n",
    "    input_cross_dim_head = 64,\n",
    "    cross_dim_head = 64,\n",
    "    latent_queries_dim_head = 64\n",
    ")\n",
    "\n",
    "model_name = 'perceiverIO_bAbi_best'\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(babi_dataset) train 180000\n",
      "vocab_size 178\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100efb88433d452ea6745dbd0a7afb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.706104 \tValidation Loss: 3.311031\n",
      "Validation loss decreased (inf --> 3.311031).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ee0a2e0dcd4e24815cd9a4a6769a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 3.272926 \tValidation Loss: 3.239527\n",
      "Validation loss decreased (3.311031 --> 3.239527).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd25d40d8de4c889d2970c69f05ba88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 3.131688 \tValidation Loss: 2.992297\n",
      "Validation loss decreased (3.239527 --> 2.992297).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d093fa59e155429ba5337d81c66b591f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 2.872207 \tValidation Loss: 2.668566\n",
      "Validation loss decreased (2.992297 --> 2.668566).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8813b2e7cce414f93c9c36bbba360bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 2.477392 \tValidation Loss: 2.354138\n",
      "Validation loss decreased (2.668566 --> 2.354138).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e1f20c7b444bbeab26cda161f7ee4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 2.277951 \tValidation Loss: 2.211176\n",
      "Validation loss decreased (2.354138 --> 2.211176).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cad046403e4bf29fb415ea23870dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 2.151721 \tValidation Loss: 2.100577\n",
      "Validation loss decreased (2.211176 --> 2.100577).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743c263b8325451d868ccc91816e7619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 2.048581 \tValidation Loss: 2.002015\n",
      "Validation loss decreased (2.100577 --> 2.002015).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31216c8ffd9f4cd5a26a4c88b5f41f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 1.955971 \tValidation Loss: 1.918928\n",
      "Validation loss decreased (2.002015 --> 1.918928).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707ca798c91e4425b5cf8547a02d713a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 1.886454 \tValidation Loss: 1.861591\n",
      "Validation loss decreased (1.918928 --> 1.861591).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1199196fd54c9e84c94aa368ac05c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 1.840335 \tValidation Loss: 1.827181\n",
      "Validation loss decreased (1.861591 --> 1.827181).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ca34432f4c4b9eadc1f68ed20587bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 1.809410 \tValidation Loss: 1.799842\n",
      "Validation loss decreased (1.827181 --> 1.799842).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21df8d08301c4edc81e2f2da1dfbbfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 1.787130 \tValidation Loss: 1.779942\n",
      "Validation loss decreased (1.799842 --> 1.779942).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97951a5a555147dc8921180e5e849bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 1.769770 \tValidation Loss: 1.765484\n",
      "Validation loss decreased (1.779942 --> 1.765484).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaea6f0f1234d3fa5336cd0d3e30ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/200:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 512 # 1800\n",
    "max_epochs = 200 # ~14h for 175309 params\n",
    "\n",
    "babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "                           vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "vocab_size = len(babi_dataset.QA.VOCAB)\n",
    "\n",
    "print('len(babi_dataset) train', len(babi_dataset))\n",
    "print('vocab_size', vocab_size)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model.to(device)\n",
    "# model.half()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "depth = 20\n",
    "criterion = PonderLoss(nn.CrossEntropyLoss(reduction='sum'), 1/10, depth, 0.01).to(device) # .half()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "# optimizer = Ranger21(model.parameters(), lr=3e-4, num_epochs=max_epochs, num_batches_per_epoch=len(babi_dataset)//batch_size+1, use_madgrad=True)\n",
    "# optimizer = Ranger(model.parameters(), lr=3e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=5, after_scheduler=scheduler)\n",
    "\n",
    "val_loss_min = float('inf')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "if not os.path.isdir(f'checkpoints/{model_name}'):\n",
    "    os.mkdir(f'checkpoints/{model_name}')\n",
    "now = datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    loss_sum = 0\n",
    "    loss_count = 0\n",
    "    model.train()\n",
    "    babi_dataset.set_mode('train')\n",
    "    train_loader = DataLoader(\n",
    "        babi_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_question\n",
    "    )\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{max_epochs}\")):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        contexts, questions, answers, tasks = data\n",
    "        contexts = contexts.long().to(device)#.half()\n",
    "        questions = questions.long().to(device)#.half()\n",
    "        answers = answers.to(device)#.half()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            p, y_hat, p_sampled, y_hat_sampled = model(contexts, questions)\n",
    "            loss = criterion(p, y_hat, answers)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        writer.add_scalars(\"losses_step\", {\"train_loss\": loss.item() / contexts.size(0)}, epoch * len(train_loader) + batch_idx)\n",
    "                \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    writer.add_scalars(\"losses_epoch\", {\"train_loss\": train_loss}, epoch)\n",
    "    train_loader_len = len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    babi_dataset.set_mode('valid')\n",
    "    val_loader = DataLoader(\n",
    "        babi_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_question\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "        \n",
    "            contexts, questions, answers, tasks = data\n",
    "            contexts = contexts.long().to(device)\n",
    "            questions = questions.long().to(device)\n",
    "            answers = answers.to(device)\n",
    "        \n",
    "            p, y_hat, p_sampled, y_hat_sampled = model(contexts, questions)\n",
    "            loss = criterion(p, y_hat, answers)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    scheduler.step(epoch, val_loss)\n",
    "    writer.add_scalars(\"losses_step\", {\"val_loss\": val_loss}, (epoch + 1) * train_loader_len - 1)    \n",
    "    writer.add_scalars(\"losses_epoch\", {\"val_loss\": val_loss}, epoch)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        val_loss\n",
    "    ))\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'valid_loss_min': val_loss,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, f'checkpoints/{model_name}/checkpoint_{now}.pt')\n",
    "    if val_loss <= val_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
    "        torch.save(checkpoint, f'checkpoints/{model_name}/best_checkpoint_{now}.pt')\n",
    "        val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "# now='29_08_2021__23_49_17'\n",
    "checkpoint_name = f'checkpoints/{model_name}/best_checkpoint_{now}.pt'\n",
    "checkpoint = torch.load(checkpoint_name)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "                           vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "babi_dataset.set_mode('test')\n",
    "test_loader = DataLoader(\n",
    "    babi_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_question\n",
    ")\n",
    "output_vocab = eval('{0: 2, 1: 6, 2: 9, 3: 15, 4: 16, 5: 19, 6: 20, 7: 22, 8: 25, 9: 154, 10: 155, 11: 156, 12: 29, 13: 157, 14: 158, 15: 160, 16: 161, 17: 162, 18: 159, 19: 164, 20: 163, 21: 165, 22: 167, 23: 43, 24: 45, 25: 173, 26: 175, 27: 177, 28: 49, 29: 54, 30: 55, 31: 60, 32: 61, 33: 62, 34: 63, 35: 64, 36: 65, 37: 66, 38: 67, 39: 68, 40: 69, 41: 70, 42: 71, 43: 72, 44: 73, 45: 74, 46: 75, 47: 76, 48: 80, 49: 82, 50: 83, 51: 84, 52: 106, 53: 108, 54: 112, 55: 113, 56: 117, 57: 120, 58: 126, 59: 127}')\n",
    "y_pred_extended = []\n",
    "y_true_extended = []\n",
    "y_pred_task = [[] for _ in range(20)]\n",
    "y_true_task = [[] for _ in range(20)]\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(tqdm(test_loader, desc=f\"Inference\")):\n",
    "        contexts, questions, answers, tasks = data\n",
    "        contexts = contexts.long().to(device)\n",
    "        questions = questions.long().to(device)\n",
    "        answers = answers.to(device)\n",
    "\n",
    "        p, y_hat, p_sampled, y_hat_sampled = model(contexts, questions)\n",
    "        # We consider a task successfully passed if â‰¥ 95% accuracy is obtained.\n",
    "        y_pred = m(y_hat_sampled.cpu())\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_pred = [babi_dataset.QA.IVOCAB[output_vocab[int(i)]] for i in y_pred]\n",
    "        y_true = [babi_dataset.QA.IVOCAB[output_vocab[int(i)]] for i in answers.cpu()]\n",
    "        y_pred_extended.extend(y_pred)\n",
    "        y_true_extended.extend(y_true)\n",
    "        for i in range(len(y_pred)):\n",
    "            y_pred_task[int(tasks[i]) - 1].append(y_pred[i])\n",
    "            y_true_task[int(tasks[i]) - 1].append(y_true[i])\n",
    "end = time.time()\n",
    "\n",
    "        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')p.shape\n",
    "\n",
    "    #print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(25,25))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "accuracy = accuracy_score(y_pred_extended, y_true_extended)\n",
    "cr = classification_report(y_true_extended, y_pred_extended)\n",
    "print(accuracy)\n",
    "print(cr)\n",
    "task_acc = []\n",
    "passed_tasks = []\n",
    "for i in range(20):\n",
    "    task_acc.append(accuracy_score(y_pred_task[i], y_true_task[i]))\n",
    "    passed_tasks.append(1 if task_acc[i] >= 0.95 else 0)\n",
    "print(task_acc)\n",
    "print('passed_tasks:', passed_tasks)\n",
    "print('no. passed_tasks:', sum(passed_tasks))\n",
    "labels = list(set(y_true_extended).intersection(set(y_true_extended)))\n",
    "cnf_matrix = confusion_matrix(y_true_extended, y_pred_extended, labels)\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=labels, title = ('Confusion Matrix'))\n",
    "plt.show()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thop import profile\n",
    "# from thop import clever_format\n",
    "\n",
    "# macs, params = profile(model, inputs=(\n",
    "#                         torch.randint(1, size=(1, 910)).type(torch.LongTensor).to(device), \n",
    "#                         torch.randint(1, size=(1, 13)).type(torch.LongTensor).to(device)))  # , \n",
    "# #                         custom_ops={YourModule: count_your_model})\n",
    "\n",
    "# macs, params = clever_format([macs, params], \"%.3f\")\n",
    "# print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers.append(model.layers[1])\n",
    "# print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-clerk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=128\n",
    "# babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "#                            vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "# vocab_size = len(babi_dataset.QA.VOCAB)\n",
    "# babi_dataset.set_mode('valid')\n",
    "# train_loader = DataLoader(\n",
    "#     babi_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate\n",
    "# )\n",
    "# max_context = 0\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, answers, tasks = data\n",
    "# #     print(contexts.shape, answers.shape)\n",
    "#     for b in range(contexts.shape[0]):\n",
    "# #         print(f'Task {int(tasks[b])}:')\n",
    "#         for i, w in enumerate(contexts[b]):\n",
    "#             if w != 0:\n",
    "#                 if i + 1 == contexts.shape[1]:\n",
    "#                     max_context = i\n",
    "#                     if i > 600:\n",
    "#                         print(f'Task {int(tasks[b])}:')\n",
    "#                         for i, w in enumerate(contexts[b]):\n",
    "#                             if w != 0:\n",
    "#                                 print(babi_dataset.QA.IVOCAB[int(w)], end=\" \")\n",
    "#                             else:\n",
    "#                                 print(f'({i} words)')\n",
    "#                                 break\n",
    "# #                 print(babi_dataset.QA.IVOCAB[int(w)], end=\" \")\n",
    "                \n",
    "#             else:\n",
    "# #                 print(f'({i} words)')\n",
    "#                 if i > max_context:\n",
    "#                     max_context = i\n",
    "#                     if i > 600:\n",
    "#                         print(f'Task {int(tasks[b])}:')\n",
    "#                         for i, w in enumerate(contexts[b]):\n",
    "#                             if w != 0:\n",
    "#                                 print(babi_dataset.QA.IVOCAB[int(w)], end=\" \")\n",
    "#                             else:\n",
    "#                                 print(f'({i} words)')\n",
    "#                                 break\n",
    "#                 break\n",
    "# #         print('\\n')\n",
    "# #         print(babi_dataset.QA.IVOCAB[int(answers[b])])\n",
    "# #         print('\\n')\n",
    "# #     break\n",
    "# print(max_context)\n",
    "\n",
    "# batch_size=128\n",
    "# babi_dataset = BabiDataset(ds_path='/home/gabriel/Documents/datasets/bAbi/en/qa{}_*', \n",
    "#                            vocab_path='/home/gabriel/Documents/datasets/bAbi/en/babi{}_vocab.pkl')\n",
    "# vocab_size = len(babi_dataset.QA.VOCAB)\n",
    "# babi_dataset.set_mode('train')\n",
    "# train_loader = DataLoader(\n",
    "#     babi_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_question\n",
    "# )\n",
    "# answer_list = []\n",
    "# max_question = 0\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, questions, answers, tasks = data\n",
    "#     answer_list += answers.tolist()\n",
    "#     for b in range(questions.shape[0]):\n",
    "#         for i, w in enumerate(questions[b]):\n",
    "#             if w == 0:\n",
    "#                 if i > max_question or i + 1 == questions.shape[1]:\n",
    "#                     max_question = i\n",
    "\n",
    "# babi_dataset.set_mode('test')\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, questions, answers, tasks = data\n",
    "#     answer_list += answers.tolist()\n",
    "#     for b in range(questions.shape[0]):\n",
    "#         for i, w in enumerate(questions[b]):\n",
    "#             if w == 0:\n",
    "#                 if i > max_question or i + 1 == questions.shape[1]:\n",
    "#                     max_question = i\n",
    "                    \n",
    "# babi_dataset.set_mode('valid')\n",
    "# for batch_idx, data in enumerate(tqdm(train_loader)):  \n",
    "#     contexts, questions, answers, tasks = data\n",
    "#     answer_list += answers.tolist()\n",
    "#     for b in range(questions.shape[0]):\n",
    "#         for i, w in enumerate(questions[b]):\n",
    "#             if w == 0:\n",
    "#                 if i > max_question or i + 1 == questions.shape[1]:\n",
    "#                     max_question = i\n",
    "    \n",
    "# print(len(set(answer_list)), set(answer_list))\n",
    "# print(max_question)\n",
    "# output_vocab = dict()\n",
    "# for i, w in enumerate(set(answer_list)):\n",
    "#     output_vocab[i] = w\n",
    "# print(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-vinyl",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
